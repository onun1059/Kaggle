{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7082,"databundleVersionId":874852,"sourceType":"competition"}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Loading packages","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.utils import shuffle\nfrom sklearn.ensemble import RandomForestClassifier\n\npd.set_option('display.max_columns', 100)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-02T19:13:26.953501Z","iopub.execute_input":"2024-07-02T19:13:26.954445Z","iopub.status.idle":"2024-07-02T19:13:26.961758Z","shell.execute_reply.started":"2024-07-02T19:13:26.954404Z","shell.execute_reply":"2024-07-02T19:13:26.960372Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Loading packages","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('../input/porto-seguro-safe-driver-prediction/train.csv')\ntest = pd.read_csv('../input/porto-seguro-safe-driver-prediction/test.csv')","metadata":{"execution":{"iopub.status.busy":"2024-07-02T19:13:26.963807Z","iopub.execute_input":"2024-07-02T19:13:26.964197Z","iopub.status.idle":"2024-07-02T19:13:36.106987Z","shell.execute_reply.started":"2024-07-02T19:13:26.964167Z","shell.execute_reply":"2024-07-02T19:13:36.10575Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data at first sight\n\nHere is an excerpt of the the data description for the competition:\n\n* Features that belong to similar groupings are tagged as such in the feature names (e.g., ind, reg, car, calc).\n* Feature names include the postfix bin to indicate binary features and cat to indicate categorical features.\n* Features without these designations are either continuous or ordinal.\n* Values of -1 indicate that the feature was missing from the observation.\n* The target columns signifies whether or not a claim was filed for that policy holder.","metadata":{}},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2024-07-02T19:13:36.109437Z","iopub.execute_input":"2024-07-02T19:13:36.109923Z","iopub.status.idle":"2024-07-02T19:13:36.15374Z","shell.execute_reply.started":"2024-07-02T19:13:36.109878Z","shell.execute_reply":"2024-07-02T19:13:36.152388Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.tail()","metadata":{"execution":{"iopub.status.busy":"2024-07-02T19:13:36.155741Z","iopub.execute_input":"2024-07-02T19:13:36.156214Z","iopub.status.idle":"2024-07-02T19:13:36.202128Z","shell.execute_reply.started":"2024-07-02T19:13:36.156172Z","shell.execute_reply":"2024-07-02T19:13:36.200907Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"we indeed see the following\n* binary variables\n* categorical variables of which the category values are integers (정수 카테고리형 변수)\n* other variables with integer or float values\n* variables with -1 representing missing values\n* the target variable and an ID variable","metadata":{}},{"cell_type":"code","source":"train.shape","metadata":{"execution":{"iopub.status.busy":"2024-07-02T19:13:36.204889Z","iopub.execute_input":"2024-07-02T19:13:36.205264Z","iopub.status.idle":"2024-07-02T19:13:36.212989Z","shell.execute_reply.started":"2024-07-02T19:13:36.205232Z","shell.execute_reply":"2024-07-02T19:13:36.211698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.drop_duplicates()\ntrain.shape\n# 중복된 열이 있는지 확인했지만 shape를 보니 59512로 동일한것으로 보아 없다.","metadata":{"execution":{"iopub.status.busy":"2024-07-02T19:13:36.214295Z","iopub.execute_input":"2024-07-02T19:13:36.214646Z","iopub.status.idle":"2024-07-02T19:13:36.976834Z","shell.execute_reply.started":"2024-07-02T19:13:36.214613Z","shell.execute_reply":"2024-07-02T19:13:36.975769Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.shape\n# target: 공지 때렸는지 안때렸는지만 없","metadata":{"execution":{"iopub.status.busy":"2024-07-02T19:13:36.978258Z","iopub.execute_input":"2024-07-02T19:13:36.978619Z","iopub.status.idle":"2024-07-02T19:13:36.985265Z","shell.execute_reply.started":"2024-07-02T19:13:36.97859Z","shell.execute_reply":"2024-07-02T19:13:36.984165Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 더미 변수(추가)\n머신러닝 알고리즘에 따라서 범주형 변수를 바로 학습 못하는 알고리즘이 있다. 그래서 그런 변수들은 더미 변수를 생성해줘야 한다. 더미 밸류란 범주형 변수들을 하나씩 풀어 이진 변수를 만들어주는 것을 말한다.\n\n예를 들어 가족형태가 single, small, big 3개 level 이였는데 어떤 알고리즘은 이것을 인식하지 못하므로 각각의 family sized.single, family sized.small, family sized,big 열을 만들어 0 또는 1을 할당하여 이진 변수로 분할 해준다.\n\n여기선 bin variable은 이미 이진 밸류여서 패스, 14개의 cat 밸류를 더미밸류로 만들어줄 수 있다.","metadata":{}},{"cell_type":"code","source":"train.info()","metadata":{"execution":{"iopub.status.busy":"2024-07-02T19:13:36.986553Z","iopub.execute_input":"2024-07-02T19:13:36.986944Z","iopub.status.idle":"2024-07-02T19:13:37.059599Z","shell.execute_reply.started":"2024-07-02T19:13:36.986904Z","shell.execute_reply":"2024-07-02T19:13:37.058295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Metadata\nTo faciliate the data management, we'll store meta-information about the variables in a DataFrame. This will be helpful when we want to select specific variables for analysis, visualization, modeling, ..\n* role: input, ID, target\n* level: nominal, interval, ordinal, binary\n* keep: True or False\n* dtype: int, float, str","metadata":{}},{"cell_type":"code","source":"train.columns","metadata":{"execution":{"iopub.status.busy":"2024-07-02T19:13:37.061079Z","iopub.execute_input":"2024-07-02T19:13:37.061486Z","iopub.status.idle":"2024-07-02T19:13:37.070209Z","shell.execute_reply.started":"2024-07-02T19:13:37.061454Z","shell.execute_reply":"2024-07-02T19:13:37.068894Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = [] \nfor f in train.columns:\n    # Definng the role\n    if f == 'target':\n        role = 'target'\n    elif f == 'id':\n        role = 'id'\n    else:\n        role = 'input'\n        \n    # Defining the level\n    if 'bin' in f or f == 'target':\n        level = 'binary'\n    elif 'cat' in f or f == 'id':\n        level = 'nominal'\n    elif train[f].dtype == float:\n        level = 'interval'\n    elif train[f].dtype == int:\n        level = 'ordinal'\n        \n    # Initialize keep to True for all variables except for id\n    keep = True\n    if f == 'id':\n        keep = False\n        \n    # Defining the data type\n    dtype = train[f].dtype\n    \n    # Creating a Dict that contains all the metadata for the variable\n    f_dict = {\n        'varname' : f,\n        'role': role,\n        'level': level,\n        'keep': keep,\n        'dtype': dtype\n    }\n    data.append(f_dict)\n\nmeta = pd.DataFrame(data, columns=['varname', 'role', 'level', 'keep', 'dtype'])\nmeta.set_index('varname', inplace=True)","metadata":{"execution":{"iopub.status.busy":"2024-07-02T19:13:37.071694Z","iopub.execute_input":"2024-07-02T19:13:37.072053Z","iopub.status.idle":"2024-07-02T19:13:37.087653Z","shell.execute_reply.started":"2024-07-02T19:13:37.072024Z","shell.execute_reply":"2024-07-02T19:13:37.086593Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data","metadata":{"execution":{"iopub.status.busy":"2024-07-02T19:13:37.09191Z","iopub.execute_input":"2024-07-02T19:13:37.092583Z","iopub.status.idle":"2024-07-02T19:13:37.118275Z","shell.execute_reply.started":"2024-07-02T19:13:37.092552Z","shell.execute_reply":"2024-07-02T19:13:37.116932Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"meta","metadata":{"execution":{"iopub.status.busy":"2024-07-02T19:13:37.119664Z","iopub.execute_input":"2024-07-02T19:13:37.120041Z","iopub.status.idle":"2024-07-02T19:13:37.149067Z","shell.execute_reply.started":"2024-07-02T19:13:37.120009Z","shell.execute_reply":"2024-07-02T19:13:37.14754Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Example to extract all nominal variables that are not dropped\nmeta[(meta.level == 'nominal') & (meta.keep)].index","metadata":{"execution":{"iopub.status.busy":"2024-07-02T19:13:37.150738Z","iopub.execute_input":"2024-07-02T19:13:37.151203Z","iopub.status.idle":"2024-07-02T19:13:37.163069Z","shell.execute_reply.started":"2024-07-02T19:13:37.15116Z","shell.execute_reply":"2024-07-02T19:13:37.161949Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame({'count' : meta.groupby(['role', 'level'])['role'].size()}).reset_index()","metadata":{"execution":{"iopub.status.busy":"2024-07-02T19:13:37.164531Z","iopub.execute_input":"2024-07-02T19:13:37.164986Z","iopub.status.idle":"2024-07-02T19:13:37.18478Z","shell.execute_reply.started":"2024-07-02T19:13:37.164953Z","shell.execute_reply":"2024-07-02T19:13:37.183506Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Descriptive statistics\n생성해둔 메타데이터로 기술(설명) 통계 구하기 수월하\n### Interval variables","metadata":{}},{"cell_type":"code","source":"v = meta[(meta.level == 'interval') & (meta.keep)].index\ntrain[v].describe()","metadata":{"execution":{"iopub.status.busy":"2024-07-02T19:13:37.186146Z","iopub.execute_input":"2024-07-02T19:13:37.186549Z","iopub.status.idle":"2024-07-02T19:13:37.507243Z","shell.execute_reply.started":"2024-07-02T19:13:37.186517Z","shell.execute_reply":"2024-07-02T19:13:37.506094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### reg variables\n* only ps_reg_03 has '-1' missing values\n* min ~ max 범위가 다른데 이부분은 이용할 분류기에 라 스케일링(standardscaler)이 필요\n\n### car variables\n* ps_car_12 & ps_car_14 모두 -1 missing values\n* 마찬가지 min ~ max 범위가 달라 스케일링 필요\n\n### calc variables\n* no -1(missing values)\n* 최댓값이 0.9fh qhdla\n* 세개 값 모두 비슷한 분포를 보임\n\n전체적으로 등간 변수의 범위가 다소 작은데, 데이터의 익명화를 위해 변환(log 등)이 이미 적용되었을 수도 있다.\n## Ordinal variables","metadata":{}},{"cell_type":"code","source":"v = meta[(meta.level == 'ordinal')&(meta.keep)].index\ntrain[v].describe()","metadata":{"execution":{"iopub.status.busy":"2024-07-02T19:13:37.508475Z","iopub.execute_input":"2024-07-02T19:13:37.508796Z","iopub.status.idle":"2024-07-02T19:13:37.858901Z","shell.execute_reply.started":"2024-07-02T19:13:37.508769Z","shell.execute_reply":"2024-07-02T19:13:37.857831Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* only ps_car_11 -1(missing values)\n* 마찬가지 스케일링 적용 필요\n\n## Binary variables","metadata":{}},{"cell_type":"code","source":"v = meta[(meta.level=='binary')&(meta.keep)].index\ntrain[v].describe()","metadata":{"execution":{"iopub.status.busy":"2024-07-02T19:13:37.860346Z","iopub.execute_input":"2024-07-02T19:13:37.860693Z","iopub.status.idle":"2024-07-02T19:13:38.183843Z","shell.execute_reply.started":"2024-07-02T19:13:37.860665Z","shell.execute_reply":"2024-07-02T19:13:38.182523Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* train set에서 target의 평균은 3.645%로 아주 불균형한 데이터 세트로 보인다\n* 평균값은 대부분 0의 값을 가지고 있을것으로 예상 가능\n\n## Handling imbalanced classes\n앞에서 말한 것처럼 target=1인 비율이 0인 비율보다 현저히 적다. 이를 통해 정확도는 높지만 실제로는 부가가치가 있는 모델로 이어질 수 있습니다. 두 가능한 전략은 이렇다.\n* oversampling records with target =1 \n* undersampling records with target = 0\n우리는 많은 training set을 갖고 있으므로 undersampling을 실행할 것이다.\n\n언더 샘플링은 너무 많은 정상 레이블 데이터를 감소시켜 정상 레이블의 경우 오히려 제대로 된 학습을 할 수 없다는 단점이 있어 잘 적용하지 않지만 여기선 언더 샘플링을 적용한다. ","metadata":{}},{"cell_type":"code","source":"desired_apriori = 0.10\n\n# Get the indices per target value\nidx_0 = train[train.target == 0].index\nidx_1 = train[train.target == 1].index\n\n# Get original number of records per target value\nnb_0 = len(train.loc[idx_0])\nnb_1 = len(train.loc[idx_1])\n\n# Calculate the undersampling rate and resulting number of records with target=0\nundersampling_rate = ((1-desired_apriori)*nb_1)/(nb_0*desired_apriori)\nundersampled_nb_0 = int(undersampling_rate*nb_0)\nprint('Rate to undersample records with target = 0: {}'.format(undersampling_rate))\nprint('Number of records with target = 0 after undersampling: {}'.format(undersampled_nb_0))\n\n# Randomly select records with target =0 to get at the desired a priori\nundersampled_idx = shuffle(idx_0, random_state = 37, n_samples=undersampled_nb_0)\n\n# Construct list with remaining indices\nidx_list = list(undersampled_idx) + list(idx_1)\n\n# Return undersample data frame\ntrain = train.loc[idx_list].reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2024-07-02T19:13:38.185372Z","iopub.execute_input":"2024-07-02T19:13:38.185726Z","iopub.status.idle":"2024-07-02T19:13:38.737541Z","shell.execute_reply.started":"2024-07-02T19:13:38.185695Z","shell.execute_reply":"2024-07-02T19:13:38.736343Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Quality checks\n\n* checking missing values\nMissings are represented as -1","metadata":{}},{"cell_type":"code","source":"vars_with_missing = []\n\nfor f in train.columns:\n    missings = train[train[f] == -1][f].count()\n    if missings > 0:\n        vars_with_missing.append(f)\n        missings_perc = missings/train.shape[0]\n        \n        print('Variable {} has {} records ({:.2%}) with missing values'.format(f, missings, missings_perc))\n        \nprint('In total, there are {} variables with missing values'.format(len(vars_with_missing)))","metadata":{"execution":{"iopub.status.busy":"2024-07-02T19:13:38.739036Z","iopub.execute_input":"2024-07-02T19:13:38.739407Z","iopub.status.idle":"2024-07-02T19:13:38.87734Z","shell.execute_reply.started":"2024-07-02T19:13:38.739375Z","shell.execute_reply":"2024-07-02T19:13:38.876026Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* ps_car_03, ps_car_05 변수는 결측값 비율이 높기때문에 제거해야 한다.\n* 다른 categorical 변수들을 위해 결측값을 -1로 남겨둔다.\n* ps_reg_03(continuous)는 18% 결측값을 가지므로 평균값으로 대체한다.\n* ps_car_11(ordinal)은 1개의 결측값을 가지므로 최빈값으로 대체한다.\n* ps_car_14(continuous)는 7% 결측값을 가지므로 평균값으로 대체한다.","metadata":{}},{"cell_type":"code","source":"# Dropping the variables with too many missing values\nvars_to_drop = ['ps_car_03_cat', 'ps_car_05_cat']\ntrain.drop(vars_to_drop, inplace=True, axis=1)\n\n# Updating the meta\nmeta.loc[(vars_to_drop), 'keep'] = False \n\n# Imputing with the mean or mode (평균, 최빈값으로 대체 )\nmean_imp = SimpleImputer(missing_values = -1, strategy='mean')\nmode_imp = SimpleImputer(missing_values = -1, strategy='most_frequent')\ntrain['ps_reg_03'] = mean_imp.fit_transform(train[['ps_reg_03']]).ravel()\ntrain['ps_car_11'] = mode_imp.fit_transform(train[['ps_car_11']]).ravel()\ntrain['ps_car_14'] = mean_imp.fit_transform(train[['ps_car_14']]).ravel()\n","metadata":{"execution":{"iopub.status.busy":"2024-07-02T19:13:38.878918Z","iopub.execute_input":"2024-07-02T19:13:38.879342Z","iopub.status.idle":"2024-07-02T19:13:38.951828Z","shell.execute_reply.started":"2024-07-02T19:13:38.879303Z","shell.execute_reply":"2024-07-02T19:13:38.95038Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Checking the cardinality of the categorical variables\n카테고리형 변수로 더미 변수를 생성하기 전에 각 카테고리 변수에 몇 개의 값들이 있는지 알아보자.","metadata":{}},{"cell_type":"code","source":"v = meta[(meta.level == 'nominal') & (meta.keep)].index\n\nfor f in v:\n    dist_values = train[f].value_counts().shape[0]\n    print('Variable {} has {} distinct values'.format(f, dist_values))","metadata":{"execution":{"iopub.status.busy":"2024-07-02T19:13:38.953402Z","iopub.execute_input":"2024-07-02T19:13:38.953834Z","iopub.status.idle":"2024-07-02T19:13:38.994907Z","shell.execute_reply.started":"2024-07-02T19:13:38.953795Z","shell.execute_reply":"2024-07-02T19:13:38.993726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"ps_car_11_cat 만 많은 값을 가지고 있다. / oliver 이 작성한 방식을 채택하겠다.","metadata":{}},{"cell_type":"code","source":"def add_noise(series, noise_level):\n    return series * (1 + noise_level * np.random.randn(len(series)))\n\ndef target_encode(trn_series = None,\n                  tst_series = None,\n                  target = None,\n                  min_samples_leaf=1,\n                  smoothing = 1,\n                  noise_level = 0):\n    \"\"\"\n    trn_series : training categoricl feature as a pd.Series\n    tst_series : test categorical feature as a pd.Series\n    target : target data as a pd.Series\n    min_samples_leaf (int) : minimum samples to take category average into account\n    smoothing (int) : smoothing effect to balance categorical average vs prior\n    \"\"\"\n    # assert : 가정설정문, 어떤 조건을 테스트하는 디버깅 보조 도구\n    assert len(trn_series) == len(target)\n    assert trn_series.name == tst_series.name\n    temp = pd.concat([trn_series, target], axis=1)\n    \n    # target 평균 계산\n    averages = temp.groupby(by=trn_series.name)[target.name].agg([\"mean\", \"count\"])\n    \n    # Smoothing 계산\n    smoothing = 1 / (1 + np.exp(-(averages[\"count\"] - min_samples_leaf) / smoothing))\n    \n    # 모든 target에 평균 적용\n    prior = target.mean()\n    \n    # count가 클수록 full_avg가 적게 고려된다.\n    averages[target.name] = prior * (1 - smoothing) + averages[\"mean\"] * smoothing\n    averages.drop([\"mean\", \"count\"], axis=1, inplace=True)\n    \n    # trn, tst series에 평균 적용\n    ft_trn_series = pd.merge(\n        trn_series.to_frame(trn_series.name),\n        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n        on=trn_series.name,\n        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n    \n    # pd.merge index를 유지하지 않으므로 저장\n    ft_trn_series.index = trn_series.index\n    ft_tst_series = pd.merge(\n        tst_series.to_frame(tst_series.name),\n        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n        on=tst_series.name,\n        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n    \n    # pd.merge index를 유지하지 않으므로 저장\n    ft_tst_series.index = tst_series.index\n    return add_noise(ft_trn_series, noise_level), add_noise(ft_tst_series, noise_level)\n    ","metadata":{"execution":{"iopub.status.busy":"2024-07-02T19:13:38.996536Z","iopub.execute_input":"2024-07-02T19:13:38.996993Z","iopub.status.idle":"2024-07-02T19:13:39.014492Z","shell.execute_reply.started":"2024-07-02T19:13:38.996952Z","shell.execute_reply":"2024-07-02T19:13:39.013404Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_encoded, test_encoded = target_encode(train[\"ps_car_11_cat\"],\n                            test[\"ps_car_11_cat\"],\n                            target=train.target,\n                            min_samples_leaf=100,\n                            smoothing=10,\n                            noise_level=0.01)\n\ntrain['ps_car_11_cat_te'] = train_encoded\ntrain.drop('ps_car_11_cat', axis=1, inplace=True)\nmeta.loc['ps_car_11_cat', 'keep'] = False # update the meta\ntest['ps_car_11_cat_te'] = test_encoded\ntest.drop('ps_car_11_cat', axis =1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2024-07-02T19:13:39.016137Z","iopub.execute_input":"2024-07-02T19:13:39.016518Z","iopub.status.idle":"2024-07-02T19:13:39.345683Z","shell.execute_reply.started":"2024-07-02T19:13:39.016487Z","shell.execute_reply":"2024-07-02T19:13:39.344518Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Exploratory Data Visualization\n\n### categorical variables\n카테고리 변수들과 target=1인 변수들을 보자","metadata":{}},{"cell_type":"code","source":"v = meta[(meta.level=='nominal') & (meta.keep)].index\n\nfor f in v:\n    plt.figure()\n    fig, ax = plt.subplots(figsize=(20,10))\n    # Calculate the percentage of targt = 1 per category value\n    cat_perc = train[[f, 'target']].groupby([f],as_index=False).mean()\n    cat_perc.sort_values(by='target', ascending=False, inplace=True)\n    \n    # Bar plot\n    # Order the bars descending on target mean\n    \n    sns.barplot(ax=ax, x=f, y='target', data = cat_perc, order=cat_perc[f])\n    plt.ylabel('% target', fontsize=18)\n    plt.xlabel(f, fontsize=18)\n    plt.tick_params(axis='both', which='major', labelsize=18)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-07-02T19:13:39.347165Z","iopub.execute_input":"2024-07-02T19:13:39.347523Z","iopub.status.idle":"2024-07-02T19:13:42.536846Z","shell.execute_reply.started":"2024-07-02T19:13:39.347491Z","shell.execute_reply":"2024-07-02T19:13:42.535682Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* 결측값이 있는 고객은 보험 청구를 요청할 확률이 훨씬 높은 것으로 보인다.(낮은 경우도 있음) 이를 최빈값으로 교체하는 등 방법이 다양하게 있을 것이다.\n\n### Interval variables\n각각 Interval variables의 상관계를 알아보자. 아래 코드는 an exmaple by Michael Waskom에 기반하여 작성했다.","metadata":{}},{"cell_type":"code","source":"def corr_heatmap(v):\n    correlations = train[v].corr()\n    \n    # Create color map ranging between two colors\n    cmap = sns.diverging_palette(220, 10, as_cmap = True)\n    \n    fig, ax = plt.subplots(figsize=(10, 10))\n    sns.heatmap(correlations, cmap=cmap, vmax=1.0, center=0, fmt='.2f',\n                square=True, linewidths=.5, annot=True, cbar_kws={\"shrink\":.75})\n    plt.show()\n    \nv = meta[(meta.level == 'interval') & (meta.keep)].index\ncorr_heatmap(v)","metadata":{"execution":{"iopub.status.busy":"2024-07-02T19:13:42.53857Z","iopub.execute_input":"2024-07-02T19:13:42.539055Z","iopub.status.idle":"2024-07-02T19:13:43.253277Z","shell.execute_reply.started":"2024-07-02T19:13:42.539013Z","shell.execute_reply":"2024-07-02T19:13:43.252138Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* 높은 상관관계를 가진 변수들:\n* ps_reg_02 & ps_reg_03(0.70)\n* ps_car_12 & ps_car_13(0.67)\n* ps_car_12 & ps_car_14(0.58)\n* ps_car_13 & ps_car_15(0.53)","metadata":{}},{"cell_type":"code","source":"# 빠른 프로세싱을 위해 sample 추출 / frac=0.1 : fraction 10% \ns = train.sample(frac=0.1)","metadata":{"execution":{"iopub.status.busy":"2024-07-02T19:13:43.254887Z","iopub.execute_input":"2024-07-02T19:13:43.255292Z","iopub.status.idle":"2024-07-02T19:13:43.286049Z","shell.execute_reply.started":"2024-07-02T19:13:43.255257Z","shell.execute_reply":"2024-07-02T19:13:43.284763Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 회귀 직선을 확인하면 피쳐들이 선형 관계가 있다는 것을 알 수 있고, hue로 회귀 직선이 target 0,1일때를 본다.\nsns.lmplot(x='ps_reg_02', y='ps_reg_03', data=s, hue='target', palette='Set1', scatter_kws={'alpha': 0.3})","metadata":{"execution":{"iopub.status.busy":"2024-07-02T19:13:43.287565Z","iopub.execute_input":"2024-07-02T19:13:43.287942Z","iopub.status.idle":"2024-07-02T19:13:45.499355Z","shell.execute_reply.started":"2024-07-02T19:13:43.2879Z","shell.execute_reply":"2024-07-02T19:13:45.498268Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# alpha : 불투명도\nsns.lmplot(x='ps_car_12', y= 'ps_car_13', data=s, hue='target', palette='Set2', scatter_kws={'alpha':0.3})","metadata":{"execution":{"iopub.status.busy":"2024-07-02T19:13:45.500962Z","iopub.execute_input":"2024-07-02T19:13:45.501422Z","iopub.status.idle":"2024-07-02T19:13:47.682417Z","shell.execute_reply.started":"2024-07-02T19:13:45.501385Z","shell.execute_reply":"2024-07-02T19:13:47.681199Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.lmplot(x='ps_car_12', y='ps_car_14', data=s, hue='target', palette='Set3', scatter_kws={'alpha':0.3})","metadata":{"execution":{"iopub.status.busy":"2024-07-02T19:13:47.689655Z","iopub.execute_input":"2024-07-02T19:13:47.690078Z","iopub.status.idle":"2024-07-02T19:13:49.997072Z","shell.execute_reply.started":"2024-07-02T19:13:47.690044Z","shell.execute_reply":"2024-07-02T19:13:49.995908Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.lmplot(x='ps_car_15', y='ps_car_13', data=s, hue='target', palette='summer_r', scatter_kws={'alpha':0.3})","metadata":{"execution":{"iopub.status.busy":"2024-07-02T19:13:49.998338Z","iopub.execute_input":"2024-07-02T19:13:49.998685Z","iopub.status.idle":"2024-07-02T19:13:52.290799Z","shell.execute_reply.started":"2024-07-02T19:13:49.998654Z","shell.execute_reply":"2024-07-02T19:13:52.289576Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Checking the Correlations between Orinal variables\n아래 heatmap을 보면 서수형 변수들에선 상관관계가 높게 나타나지 않는다. 하지만 우리는 목표값별로 그룹화할 때 분포가 어떻게 되는지 알아볼수 있다.","metadata":{}},{"cell_type":"code","source":"v = meta[(meta.level == 'ordinal') & (meta.keep)].index\ncorr_heatmap(v)","metadata":{"execution":{"iopub.status.busy":"2024-07-02T19:13:52.292223Z","iopub.execute_input":"2024-07-02T19:13:52.292574Z","iopub.status.idle":"2024-07-02T19:13:53.483414Z","shell.execute_reply.started":"2024-07-02T19:13:52.292545Z","shell.execute_reply":"2024-07-02T19:13:53.482271Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature engineering\n\n### Creating dummy variables\n카테고리형 변수에서 1과 2는 값이 2배임을 의미하진 않는다. 그러므로 더미형 변수를 만들어줘야 한다. 첫번째 더미 변수를 삭제하는 이유는 원래 변수의 범주에 대해 생성된 다른 다른 더미 변수에서 파생될수 있기 때문이다.","metadata":{}},{"cell_type":"code","source":"v = meta[(meta.level=='nominal') & (meta.keep)].index\nprint('Before dummification we have {} variables in train'.format(train.shape[1]))\ntrain = pd.get_dummies(train, columns=v, drop_first=True)\nprint('After dummification we have {} variables in train'.format(train.shape[1]))","metadata":{"execution":{"iopub.status.busy":"2024-07-02T19:13:53.48481Z","iopub.execute_input":"2024-07-02T19:13:53.48519Z","iopub.status.idle":"2024-07-02T19:13:53.70804Z","shell.execute_reply.started":"2024-07-02T19:13:53.485154Z","shell.execute_reply":"2024-07-02T19:13:53.706702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* drop_first 에 대한 추가 설명\n\n타이타닉 예제에서 예를 들면 Pclass 1, 2, 3이 있었고, 이에 더미 변수를 생성하면 Pclass_1, Pclass_2, Pclass_3이 나온다. 여기서 drop_first를 하면 Pclass_2, Pclass_3만 나온다. 이렇게 해도 두 값이 0이라면 Pclass가 1임을 알수 있기 때문이다.\n\n### Creating interaction variables","metadata":{}},{"cell_type":"code","source":"v = meta[(meta.level == 'interval') & (meta.keep)].index\npoly = PolynomialFeatures(degree=2, interaction_only=False, include_bias=False)\ninteractions = pd.DataFrame(data=poly.fit_transform(train[v]), columns=poly.get_feature_names_out(v))\ninteractions.drop(v, axis=1, inplace=True)\nprint(f'교호작용 전에 train 세트에 있는 변수의 갯수: {train.shape[1]}')\ntrain = pd.concat([train, interactions], axis=1)\nprint('After dummification we have {} variables in train'.format(train.shape[1]))","metadata":{"execution":{"iopub.status.busy":"2024-07-02T19:13:53.709626Z","iopub.execute_input":"2024-07-02T19:13:53.710002Z","iopub.status.idle":"2024-07-02T19:13:54.062247Z","shell.execute_reply.started":"2024-07-02T19:13:53.709971Z","shell.execute_reply":"2024-07-02T19:13:54.061127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature Selection\n\n### Remooving Features with low or zero variance\n\n분산이 없거나 매우 낮은 피쳐럴 제거할 수 있는데, sklearn의 VarianceThreshold를 이용해서 제거할 수 있다. 기본적으로 분산이 0인 피쳐를 제거하는데 이전 단계에서 분산이 0인 변수가 없었으므로, 1% 미만인 피쳐를 제거하게 설정할 수 있다.\n\nVectorize는 매트릭스 구조의 데이터의 연산을 일괄적으로 처리할 수 있도록 Series, Dataframe, array 등과 같이 시퀀스 형 자료를 함수의 매개변수로 포함시킬 수 있게 하는 것을 말한다.","metadata":{}},{"cell_type":"code","source":"selector = VarianceThreshold(threshold = .01)\nselector.fit(train.drop(['id', 'target'], axis=1)) # Fit to train without id and target variables\n\nf = np.vectorize(lambda x: not x) # Function to toggle boolean array elements\n\nv = train.drop(['id', 'target'], axis=1).columns[f(selector.get_support())]\nprint('{} variables have too low variance'.format(len(v)))\nprint('These variables are {}'.format(list(v)))","metadata":{"execution":{"iopub.status.busy":"2024-07-02T19:13:54.063679Z","iopub.execute_input":"2024-07-02T19:13:54.064066Z","iopub.status.idle":"2024-07-02T19:13:54.793022Z","shell.execute_reply.started":"2024-07-02T19:13:54.064035Z","shell.execute_reply":"2024-07-02T19:13:54.79184Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"만약 분산을 바탕으로 선택하면 많은 변수가 없어질 것이다. 그러나 우리가 지금 변수를 많이 갖고 있기에 분류기가 선택하도록 하자. sklearn의 SelectFromModel 메서드를 사용하면 최상의 피쳐를 선택하게 할 수 있다. 아래에서 랜덤 포레스트를 통해 방법을 설명한다.\n\n### Selecting Features with a Random Forest and SelectFromModel\n\n랜덤 포레스트의 feature importances를 이용해서 선택하도록 하자. SelectFromModel로 유지할 변수의 수를 지정할 수 있고, feature importances에 대한 임계값을 수동으로 설정할 수 있다. 여기선 간단하게 상위 50%만 설정한다.","metadata":{}},{"cell_type":"code","source":"X_train = train.drop(['id', 'target'], axis=1)\ny_train = train['target']\n\nfeat_labels = X_train.columns\n\nrf = RandomForestClassifier(n_estimators=1000, random_state=0, n_jobs=-1)\nrf.fit(X_train, y_train)\nimportances = rf.feature_importances_\n\nindices = np.argsort(rf.feature_importances_)[::-1]\n\nfor f in range(X_train.shape[1]):\n    print(\"%2d) %-*s %f\" % (f + 1, 30, feat_labels[indices[f]], importances[indices[f]]))","metadata":{"execution":{"iopub.status.busy":"2024-07-02T19:16:18.036436Z","iopub.execute_input":"2024-07-02T19:16:18.036885Z","iopub.status.idle":"2024-07-02T19:29:06.136674Z","shell.execute_reply.started":"2024-07-02T19:16:18.036829Z","shell.execute_reply":"2024-07-02T19:29:06.13533Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sfm = SelectFromModel(rf, threshold='median', prefit=True)\nprint(f'Selection 전 feature 수: {X_train.shape[1]}' )\nn_features = sfm.transform(X_train).shape[1]\nprint(f'Selection 후 feature 수: {n_features}')\nselected_vars = list(feat_labels[sfm.get_support()])","metadata":{"execution":{"iopub.status.busy":"2024-07-02T20:06:21.763781Z","iopub.execute_input":"2024-07-02T20:06:21.764879Z","iopub.status.idle":"2024-07-02T20:06:23.720892Z","shell.execute_reply.started":"2024-07-02T20:06:21.764814Z","shell.execute_reply":"2024-07-02T20:06:23.719626Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = train[selected_vars + ['target']]","metadata":{"execution":{"iopub.status.busy":"2024-07-02T20:06:46.405747Z","iopub.execute_input":"2024-07-02T20:06:46.406494Z","iopub.status.idle":"2024-07-02T20:06:46.469542Z","shell.execute_reply.started":"2024-07-02T20:06:46.406458Z","shell.execute_reply":"2024-07-02T20:06:46.468353Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature scaling\n앞서 언급한 것과 같이 train set에 standard scaling을 할 수 있고, 이는 몇몇 분류기의 성능을 향상시킬 수 있다.","metadata":{}},{"cell_type":"code","source":"scaler = StandardScaler()\nscaler.fit_transform(train.drop(['target'], axis=1))","metadata":{"execution":{"iopub.status.busy":"2024-07-02T20:09:48.175609Z","iopub.execute_input":"2024-07-02T20:09:48.176539Z","iopub.status.idle":"2024-07-02T20:09:48.592187Z","shell.execute_reply.started":"2024-07-02T20:09:48.176489Z","shell.execute_reply":"2024-07-02T20:09:48.591032Z"},"trusted":true},"execution_count":null,"outputs":[]}]}